\cite{Cutkosky2017} gives a new frontier \textit{lower} bounds of regret for online optimization problems in the case of without any prior information (e.g., the diameter of $W$, the bound of gradients $L_{\max}$ of loss function, hyper parameters, etc.). It proves that the lower bound they provide is already the optimal one if with no prior information. 

It starts from constructing an adversarial sequence of subgradients $g_t \in \mathbb{R}$. The sequence set $g_t=-1$ for $T-1$ iterations, and $g_T=O(k\sqrt{T})$. This strategy forces the algorithm to experience regret that is exponential in $\sqrt{T}/k$. Then it expresses $\sqrt{T}/k$ as a constant multiple of $\frac{1}{k^2}L_t/L_{t-1}$, where $L_t = \max_{t^{'} \leq t} \vert g_t \vert$, capturing the algorithm's sensitivity to the big jump in the gradients between $T-1$ and $T$ in the adversarial sequence. As a result, the regret at a certain benchmark point $\mu$ bounded by:
\begin{equation}\label{eq: OLWPI_1}
    \begin{aligned}
    R_T(\mu) & = \sum_{t=1}^{T}g_t w_t - g_t\mu \\
             & \geq k\Vert\mu\Vert L_{\max}\log(T\Vert \mu \Vert + 1)\sqrt{T} + \frac{L_{\max}}{T-1}\exp \bigg(\frac{\sqrt{T-1}}{8k}\bigg) \\
             & \geq k\Vert\mu\Vert L_{\max}\log(T\Vert \mu \Vert + 1)\sqrt{T} + \max_{t \leq T}L_{\max}\frac{L_{t-1}^2}{{\Vert g \Vert}_{1:t-1}^2}\exp\bigg[\frac{1}{2}\bigg(\frac{L_t/L_{t-1}}{288k^2}\bigg)\bigg]
    \end{aligned}
\end{equation}

If there exists an algorithm whose upper bound matches above lower bound for any $k$, above inequality \ref{eq: OLWPI_1} tells us it guarantees an optimal lower bound without prior information so long as $L_t$ does not increase to quickly.

To extend the bound to $\gamma$ dimension, it sets $g_T=O(\gamma k^{1/\gamma}T^{1-1/{2\gamma}})$ and results a penalty that is exponential in $(\sqrt{T}/k)^{1/\gamma}$, which can be expressed as a multiple of $(\frac{1}{\gamma k^2}L_t/L_{t-1})^{1/(2\gamma-1)}$. The regret lower bound can be formulated as following:
\begin{equation}\label{eq: OLWPI_2}
    \begin{aligned}
    R_T(\mu) & = \sum_{t=1}^{T}g_t w_t - g_t\mu \\
             & \geq k\Vert\mu\Vert L_{\max}\log^\gamma(T\Vert \mu \Vert + 1)\sqrt{T} + \frac{L_{\max}}{T-1}\exp \bigg(\frac{(T-1)^{1/{2\gamma}}}{2(4k)^{1/\gamma}}\bigg) \\
             & \geq k\Vert\mu\Vert L_{\max}\log^\gamma(T\Vert \mu \Vert + 1)\sqrt{T} + \max_{t \leq T}L_{\max}\frac{L_{t-1}^2}{{\Vert g \Vert}_{1:t-1}^2}\exp\bigg[\frac{1}{2}\bigg(\frac{L_t/L_{t-1}}{288\gamma k^2}\bigg)^{1/(2\gamma-1)}\bigg]
    \end{aligned}
\end{equation}

\hl{After some number of iterations, the exponential term in the second inequality no longer grows with $T$, and the adversarial strategy is not available because it requires a choice of $L_{\max}$ that depends on T.} Therefore an algorithm that guarantees regret matching the second inequality in \ref{eq: OLWPI_2} for some $k$ and $\gamma$ will obtain an asymptotic dependence on $T$ that is only $\log^\gamma (T)\sqrt{T}$.

The following of the paper derives algorithms that match any point on the frontier without prior information.

It uses Follow-the-Regularized-Leader (FTRL) framework (\cite{shalev2012online}) to find a way whose \textit{upper} bound of regret can be aligned with the previous \textit{lower} bound, which means they decrease the upper bounds to the already known lower bounds (i.e., optimal upper bound of regret). At the end, they give an explicit description of this algorithm  which is called FreeRex (\textbf{Free}-Information \textbf{R}egret \textbf{ex}ponetial updates, see algorithm \ref{alg:freeRex}).
\begin{algorithm}
\caption{FreeRex}
\label{alg:freeRex}
{\bfseries Input:} $k$

{\bfseries Initialize:} $\frac{1}{\eta_0^2} \leftarrow 0, a_0 \leftarrow 0, w_1 \leftarrow 0, L_0 \leftarrow 0, \psi(w) = (\Vert w \Vert + 1)\log(\Vert w \Vert + 1) - \Vert w \Vert.$
\begin{algorithmic}[1]
\For {$t=1$ {\bfseries to} $T$}
    \State {Play $w_t$, receive subgradient $g_t \in \partial{l_t(w_t)}$.}
    \State {$L_t \leftarrow \max(L_{t-1}, \Vert g_t \Vert)$.}
    \State{$\frac{1}{\eta_{t}^2} \leftarrow \max\Big(\frac{1}{\eta_{t-1}^2}+2{\Vert g_t \Vert}^2, L_t\Vert g_{1:t}\Vert\Big)$.}
    \State {$a_t \leftarrow \max(a_{t-1}, 1/{(L_t\eta_t)}^2)$.}
    \State {//Set $w_{t+1}$ using FTRL update}
    \State {$w_{t+1} \leftarrow -\frac{g_{1:t}}{a_t\Vert g_{1:t}\Vert}\bigg[\exp\Big(\frac{\eta_t\Vert g_{1:t}\Vert}{k}\Big)-1\bigg]$.}
\EndFor
\end{algorithmic}
\end{algorithm}