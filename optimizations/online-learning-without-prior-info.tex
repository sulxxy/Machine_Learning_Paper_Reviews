\cite{Cutkosky2017} gives a new frontier \textit{lower} bounds of regret for online optimization problems in the case of without any prior information (i.e., the diameter of $W$ or the bound of gradients $L_{\max}$ of loss function). They prove that the lower bound they provide is already the optimal one if with no prior information. 

The second part of the paper uses Follow-the-Regularized-Leader (FTRL) framework (\cite{shalev2012online}) to find a way whose \textit{upper} bound of regret can be aligned with the previous \textit{lower} bound, which means they decrease the upper bounds to the already known lower bounds (i.e., optimal upper bound of regret). At the end, they give a explicit description of this algorithm  which is called FreeRex (\textbf{Free}-Information \textbf{R}egret \textbf{ex}ponetial updates, see algorithm \ref{alg:freeRex}).
\begin{algorithm}
\caption{FreeRex}
\label{alg:freeRex}
{\bfseries Input:} $k$

{\bfseries Initialize:} $\frac{1}{\eta_0^2} \leftarrow 0, a_0 \leftarrow 0, w_1 \leftarrow 0, L_0 \leftarrow 0, \psi(w) = (\Vert w \Vert + 1)\log(\Vert w \Vert + 1) - \Vert w \Vert.$
\begin{algorithmic}[1]
\For {$t=1$ {\bfseries to} $T$}
    \State {Play $w_t$, receive subgradient $g_t \in \partial{l_t(w_t)}$.}
    \State {$L_t \leftarrow \max(L_{t-1}, \Vert g_t \Vert)$.}
    \State{$\frac{1}{\eta_{t}^2} \leftarrow \max\Big(\frac{1}{\eta_{t-1}^2}+2{\Vert g_t \Vert}^2, L_t\Vert g_{1:t}\Vert\Big)$.}
    \State {$a_t \leftarrow \max(a_{t-1}, 1/{(L_t\eta_t)}^2)$.}
    \State {//Set $w_{t+1}$ using FTRL update}
    \State {$w_{t+1} \leftarrow -\frac{g_{1:t}}{a_t\Vert g_{1:t}\Vert}\bigg[\exp\Big(\frac{\eta_t\Vert g_{1:t}\Vert}{k}\Big)-1\bigg]$.}
\EndFor
\end{algorithmic}
\end{algorithm}